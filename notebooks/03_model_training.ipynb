{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642187fb",
   "metadata": {},
   "source": [
    "# EMNIST CNN Model Training\n",
    "\n",
    "This notebook trains the CNN model for EMNIST character recognition.\n",
    "\n",
    "**Training Configuration:**\n",
    "- **Batch size**: 128\n",
    "- **Max epochs**: 30\n",
    "- **Optimizer**: Adam (lr=0.001)\n",
    "- **Loss**: Categorical crossentropy\n",
    "- **Metrics**: Accuracy, Top-5 accuracy\n",
    "- **Callbacks**: Early stopping, model checkpoint, learning rate reduction\n",
    "- **Data augmentation**: Rotation (Â±15Â°), shifts (Â±10%), zoom (Â±10%)\n",
    "\n",
    "**Target**: â‰¥85% test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343e24c",
   "metadata": {},
   "source": [
    "## é‡è¦æç¤ºï¼šè¨˜æ†¶é«”ç®¡ç†\n",
    "\n",
    "ç”±æ–¼ EMNIST æ•¸æ“šé›†è¼ƒå¤§ï¼ˆ~697K è¨“ç·´æ¨£æœ¬ï¼‰ï¼Œå®Œæ•´è¨“ç·´å¯èƒ½éœ€è¦è¼ƒå¤šè¨˜æ†¶é«”ã€‚\n",
    "\n",
    "**é¸é … 1**: ä½¿ç”¨æ•¸æ“šå­é›†å¿«é€Ÿæ¸¬è©¦ï¼ˆæŽ¨è–¦å…ˆæ¸¬è©¦ï¼‰\n",
    "**é¸é … 2**: ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†ï¼ˆéœ€è¦è¶³å¤  RAMï¼Œç´„ 8GB+ï¼‰\n",
    "\n",
    "æœ¬ notebook æä¾›å…©ç¨®æ¨¡å¼ï¼Œå¯æ ¹æ“šæ‚¨çš„ç¡¬é«”é¸æ“‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# è¨˜æ†¶é«”ç®¡ç†é…ç½®\n",
    "# ========================================\n",
    "\n",
    "# é¸æ“‡è¨“ç·´æ¨¡å¼\n",
    "USE_SUBSET = True  # True: ä½¿ç”¨æ•¸æ“šå­é›† (è¨˜æ†¶é«”å‹å¥½), False: ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†\n",
    "\n",
    "if USE_SUBSET:\n",
    "    SUBSET_SIZE = 50000  # ä½¿ç”¨ 50K è¨“ç·´æ¨£æœ¬ï¼ˆç´„ä½” 7%ï¼‰\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 20\n",
    "    print(\"âš  ä½¿ç”¨æ•¸æ“šå­é›†æ¨¡å¼\")\n",
    "    print(f\"  è¨“ç·´æ¨£æœ¬: {SUBSET_SIZE:,} (ç´„ 7% å®Œæ•´æ•¸æ“š)\")\n",
    "    print(f\"  é æœŸæº–ç¢ºçŽ‡: ~80-83% (å®Œæ•´æ•¸æ“šå¯é” 85%+)\")\n",
    "else:\n",
    "    SUBSET_SIZE = None\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 30\n",
    "    print(\"âœ“ ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†æ¨¡å¼\")\n",
    "    print(f\"  éœ€è¦è¨˜æ†¶é«”: ~8GB+ RAM\")\n",
    "    print(f\"  è¨“ç·´æ™‚é–“: 20-40 åˆ†é˜\")\n",
    "\n",
    "print(f\"\\né…ç½®:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c886edf",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178006fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules\n",
    "from src.data.dataset import load_emnist\n",
    "from src.preprocessing.preprocessing import (\n",
    "    preprocess_data,\n",
    "    create_train_val_split,\n",
    "    create_data_augmentation_generator\n",
    ")\n",
    "from src.models.cnn import create_cnn_model, compile_model\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f25e56",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EMNIST dataset\n",
    "print(\"Loading EMNIST ByClass dataset...\")\n",
    "start_time = time.time()\n",
    "x_train_full, y_train_full, x_test, y_test = load_emnist()\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ Dataset loaded in {load_time:.1f}s\")\n",
    "print(f\"  Full training set: {x_train_full.shape[0]:,} samples\")\n",
    "\n",
    "# ä½¿ç”¨å­é›†æˆ–å®Œæ•´æ•¸æ“š\n",
    "if USE_SUBSET and SUBSET_SIZE is not None:\n",
    "    # éš¨æ©Ÿé¸æ“‡å­é›†ï¼ˆstratified samplingï¼‰\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    indices = np.arange(len(x_train_full))\n",
    "    subset_indices, _ = tts(\n",
    "        indices, \n",
    "        train_size=SUBSET_SIZE, \n",
    "        stratify=y_train_full, \n",
    "        random_state=42\n",
    "    )\n",
    "    x_train = x_train_full[subset_indices]\n",
    "    y_train = y_train_full[subset_indices]\n",
    "    print(f\"  âš  Using subset: {x_train.shape[0]:,} samples ({x_train.shape[0]/x_train_full.shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    # é‡‹æ”¾è¨˜æ†¶é«”\n",
    "    del x_train_full, y_train_full\n",
    "    import gc\n",
    "    gc.collect()\n",
    "else:\n",
    "    x_train = x_train_full\n",
    "    y_train = y_train_full\n",
    "    print(f\"  âœ“ Using full dataset: {x_train.shape[0]:,} samples\")\n",
    "\n",
    "print(f\"  Test: {x_test.shape[0]:,} samples\")\n",
    "print(f\"  Image shape: {x_train.shape[1:]}\")\n",
    "print(f\"  Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42680df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (normalize, reshape, one-hot encode) - Memory efficient\n",
    "print(\"\\nPreprocessing data (in-place, memory-efficient)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Normalize in-place\n",
    "print(\"  Normalizing...\")\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Reshape in-place\n",
    "print(\"  Reshaping...\")\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# One-hot encode\n",
    "print(\"  One-hot encoding...\")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=62)\n",
    "y_test = to_categorical(y_test, num_classes=62)\n",
    "\n",
    "prep_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ Preprocessing complete in {prep_time:.1f}s\")\n",
    "print(f\"  Training shape: {x_train.shape}\")\n",
    "print(f\"  Training labels shape: {y_train.shape}\")\n",
    "print(f\"  Value range: [{x_train.min():.2f}, {x_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13be933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split (85/15)\n",
    "print(\"\\nCreating train/validation split...\")\n",
    "x_train, x_val, y_train, y_val = create_train_val_split(\n",
    "    x_train, y_train, val_size=0.15, random_state=42, stratify=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Split complete\")\n",
    "print(f\"  Training: {x_train.shape[0]:,} samples (85%)\")\n",
    "print(f\"  Validation: {x_val.shape[0]:,} samples (15%)\")\n",
    "print(f\"  Test: {x_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc69259",
   "metadata": {},
   "source": [
    "## 3. Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0521e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the CNN model\n",
    "print(\"Creating CNN model...\")\n",
    "model = create_cnn_model()\n",
    "model = compile_model(model, learning_rate=0.001)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "model.summary()\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count parameters\n",
    "trainable = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable = sum([tf.size(w).numpy() for w in model.non_trainable_weights])\n",
    "print(f\"\\nâœ“ Model created\")\n",
    "print(f\"  Total parameters: {trainable + non_trainable:,}\")\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "print(f\"  Non-trainable: {non_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4343e",
   "metadata": {},
   "source": [
    "## 4. Setup Data Augmentation and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data augmentation\n",
    "datagen = create_data_augmentation_generator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "print(\"âœ“ Data augmentation configured\")\n",
    "print(\"  Rotation: Â±15Â°\")\n",
    "print(\"  Shifts: Â±10%\")\n",
    "print(\"  Zoom: Â±10%\")\n",
    "\n",
    "# Setup callbacks\n",
    "model_path = Path('../models/emnist_cnn_v1.keras')\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(model_path),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nâœ“ Callbacks configured\")\n",
    "print(\"  Model checkpoint: Save best model by val_accuracy\")\n",
    "print(\"  Early stopping: Patience 5 epochs\")\n",
    "print(\"  Reduce LR: Factor 0.5, patience 3 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3541b3a",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "**Note**: This will take approximately 15-30 minutes depending on hardware.\n",
    "Training ~594K images for up to 30 epochs with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration (ä½¿ç”¨ä¸Šé¢å®šç¾©çš„ BATCH_SIZE å’Œ EPOCHS)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max epochs: {EPOCHS}\")\n",
    "print(f\"  Training samples: {x_train.shape[0]:,}\")\n",
    "print(f\"  Steps per epoch: {len(x_train) // BATCH_SIZE}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train the model\n",
    "training_start = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"âœ“ Training completed in {training_time/60:.1f} minutes\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc45ee3",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_results = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "test_loss = test_results[0]\n",
    "test_accuracy = test_results[1]\n",
    "test_top5_accuracy = test_results[2]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test top-5 accuracy: {test_top5_accuracy*100:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "target_met = test_accuracy >= 0.85\n",
    "print(f\"\\nTarget (â‰¥85% accuracy): {'âœ“ MET' if target_met else 'âœ— NOT MET'}\")\n",
    "\n",
    "if target_met:\n",
    "    print(\"ðŸŽ‰ Model achieves the target accuracy!\")\n",
    "else:\n",
    "    print(f\"âš  Model needs {(0.85 - test_accuracy)*100:.2f}% more accuracy to meet target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ca999",
   "metadata": {},
   "source": [
    "## 7. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14053a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Top-5 Accuracy\n",
    "axes[1, 0].plot(history.history['top_5_accuracy'], label='Training Top-5', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_top_5_accuracy'], label='Validation Top-5', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Top-5 Accuracy', fontsize=12)\n",
    "axes[1, 0].set_title('Top-5 Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate (if available)\n",
    "if 'lr' in history.history:\n",
    "    axes[1, 1].plot(history.history['lr'], linewidth=2, color='green')\n",
    "    axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Learning rate not tracked', \n",
    "                     ha='center', va='center', fontsize=14)\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"  Best validation accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"  Final validation accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Final training accuracy: {history.history['accuracy'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f442be4",
   "metadata": {},
   "source": [
    "## 8. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57351e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history to JSON\n",
    "history_path = Path('../models/emnist_cnn_v1_history.json')\n",
    "\n",
    "history_dict = {\n",
    "    'history': {k: [float(v) for v in vals] for k, vals in history.history.items()},\n",
    "    'config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'learning_rate': 0.001,\n",
    "        'val_size': 0.15,\n",
    "        'use_augmentation': True,\n",
    "        'use_subset': USE_SUBSET,\n",
    "        'subset_size': SUBSET_SIZE if USE_SUBSET else None,\n",
    "        'training_time_seconds': training_time\n",
    "    },\n",
    "    'test_results': {\n",
    "        'test_loss': float(test_loss),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_top5_accuracy': float(test_top5_accuracy)\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Training history saved to {history_path}\")\n",
    "print(f\"âœ“ Best model saved to {model_path}\")\n",
    "\n",
    "if USE_SUBSET:\n",
    "    print(f\"\\nâš  Note: Trained with {SUBSET_SIZE:,} samples subset\")\n",
    "    print(f\"  For â‰¥85% accuracy, retrain with USE_SUBSET=False\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
