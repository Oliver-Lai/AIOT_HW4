{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c37d68",
   "metadata": {},
   "source": [
    "# EMNIST Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation of the trained EMNIST CNN model.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Overall test accuracy and top-5 accuracy\n",
    "- Per-class precision, recall, F1-score\n",
    "- Confusion matrix visualization\n",
    "- Commonly confused character pairs analysis\n",
    "- Sample predictions with confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00ec29",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ad8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Import custom modules\n",
    "from src.data.dataset import load_emnist\n",
    "from src.utils.label_mapping import load_label_mapping\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "\n",
    "# Load trained model\n",
    "MODEL_PATH = '../models/emnist_cnn_v1.keras'\n",
    "\n",
    "if Path(MODEL_PATH).exists():\n",
    "    print(f\"\\nLoading model from {MODEL_PATH}...\")\n",
    "    model = keras.models.load_model(MODEL_PATH)\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "    print(f\"  Input shape: {model.input_shape}\")\n",
    "    print(f\"  Output shape: {model.output_shape}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Model not found at {MODEL_PATH}\")\n",
    "    print(\"Please train the model first using notebooks/03_model_training.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b65ba",
   "metadata": {},
   "source": [
    "## 2. Load Test Data and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "print(\"Loading EMNIST test set...\")\n",
    "_, _, x_test, y_test_labels = load_emnist()\n",
    "\n",
    "# Preprocess\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_labels, 62)\n",
    "\n",
    "print(f\"✓ Test set loaded: {x_test.shape[0]:,} samples\")\n",
    "\n",
    "# Load label mapping\n",
    "label_mapping = load_label_mapping()\n",
    "class_names = [label_mapping[i] for i in range(62)]\n",
    "print(f\"✓ Label mapping loaded: {len(class_names)} classes\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions on test set...\")\n",
    "y_pred_probs = model.predict(x_test, verbose=0)\n",
    "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "print(\"✓ Predictions complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd6d70",
   "metadata": {},
   "source": [
    "## 3. Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate overall metrics\n",
    "test_results = model.evaluate(x_test, y_test, verbose=0)\n",
    "test_loss = test_results[0]\n",
    "test_accuracy = test_results[1]\n",
    "test_top5 = test_results[2] if len(test_results) > 2 else None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OVERALL PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "if test_top5:\n",
    "    print(f\"Top-5 Accuracy: {test_top5*100:.2f}%\")\n",
    "\n",
    "# Compute accuracy per category\n",
    "print(\"\\nAccuracy by Character Category:\")\n",
    "digits_mask = y_test_labels < 10\n",
    "uppercase_mask = (y_test_labels >= 10) & (y_test_labels < 36)\n",
    "lowercase_mask = y_test_labels >= 36\n",
    "\n",
    "digits_acc = np.mean(y_pred_labels[digits_mask] == y_test_labels[digits_mask])\n",
    "uppercase_acc = np.mean(y_pred_labels[uppercase_mask] == y_test_labels[uppercase_mask])\n",
    "lowercase_acc = np.mean(y_pred_labels[lowercase_mask] == y_test_labels[lowercase_mask])\n",
    "\n",
    "print(f\"  Digits (0-9): {digits_acc*100:.2f}%\")\n",
    "print(f\"  Uppercase (A-Z): {uppercase_acc*100:.2f}%\")\n",
    "print(f\"  Lowercase (a-z): {lowercase_acc*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b328eaf5",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733be8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "print(\"Computing confusion matrix...\")\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "print(f\"✓ Confusion matrix computed: {conf_matrix.shape}\")\n",
    "\n",
    "# Normalize confusion matrix\n",
    "conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(20, 18))\n",
    "\n",
    "sns.heatmap(\n",
    "    conf_matrix_norm,\n",
    "    annot=False,\n",
    "    fmt='.2f',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Normalized Count'},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=14)\n",
    "ax.set_ylabel('True Label', fontsize=14)\n",
    "ax.set_title('Confusion Matrix - EMNIST 62 Classes', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix\n",
    "save_path = Path('../models/confusion_matrix.png')\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"✓ Confusion matrix saved to {save_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print diagonal (correct predictions)\n",
    "diagonal_acc = np.diag(conf_matrix_norm)\n",
    "print(f\"\\nPer-class accuracy statistics:\")\n",
    "print(f\"  Mean: {diagonal_acc.mean()*100:.2f}%\")\n",
    "print(f\"  Std: {diagonal_acc.std()*100:.2f}%\")\n",
    "print(f\"  Min: {diagonal_acc.min()*100:.2f}% (class '{class_names[diagonal_acc.argmin()]}')\")\n",
    "print(f\"  Max: {diagonal_acc.max()*100:.2f}% (class '{class_names[diagonal_acc.argmax()]}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c62e7e",
   "metadata": {},
   "source": [
    "## 5. Commonly Confused Character Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find commonly confused pairs\n",
    "confused_pairs = []\n",
    "for i in range(62):\n",
    "    for j in range(62):\n",
    "        if i != j and conf_matrix[i, j] > 50:  # Threshold for \"commonly confused\"\n",
    "            confused_pairs.append({\n",
    "                'true_class': class_names[i],\n",
    "                'predicted_class': class_names[j],\n",
    "                'count': int(conf_matrix[i, j]),\n",
    "                'percentage': float(conf_matrix[i, j] / conf_matrix[i].sum() * 100)\n",
    "            })\n",
    "\n",
    "# Sort by count\n",
    "confused_pairs.sort(key=lambda x: x['count'], reverse=True)\n",
    "\n",
    "print(f\"Found {len(confused_pairs)} commonly confused pairs (>50 confusions)\\n\")\n",
    "print(\"Top 15 Most Confused Character Pairs:\")\n",
    "print(\"=\"*70)\n",
    "for i, pair in enumerate(confused_pairs[:15], 1):\n",
    "    print(f\"{i:2d}. '{pair['true_class']}' → '{pair['predicted_class']}': \"\n",
    "          f\"{pair['count']:4d} times ({pair['percentage']:5.2f}%)\")\n",
    "\n",
    "# Visualize top confused pairs\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_pairs = confused_pairs[:15]\n",
    "labels = [f\"'{p['true_class']}'→'{p['predicted_class']}'\" for p in top_pairs]\n",
    "counts = [p['count'] for p in top_pairs]\n",
    "\n",
    "bars = ax.barh(range(len(top_pairs)), counts, color='coral', edgecolor='darkred', alpha=0.7)\n",
    "ax.set_yticks(range(len(top_pairs)))\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Number of Confusions', fontsize=12)\n",
    "ax.set_title('Top 15 Most Confused Character Pairs', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "    ax.text(count + 20, i, str(count), va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cad064",
   "metadata": {},
   "source": [
    "## 6. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(\n",
    "    y_test_labels, \n",
    "    y_pred_labels,\n",
    "    target_names=class_names,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Extract per-class metrics\n",
    "per_class_f1 = [report[name]['f1-score'] for name in class_names if name in report]\n",
    "per_class_precision = [report[name]['precision'] for name in class_names if name in report]\n",
    "per_class_recall = [report[name]['recall'] for name in class_names if name in report]\n",
    "\n",
    "# Plot per-class F1-scores\n",
    "fig, axes = plt.subplots(3, 1, figsize=(18, 15))\n",
    "\n",
    "# F1-Score\n",
    "axes[0].bar(range(62), per_class_f1, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes[0].set_xticks(range(62))\n",
    "axes[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('Per-Class F1-Score', fontsize=14, fontweight='bold')\n",
    "axes[0].axhline(y=np.mean(per_class_f1), color='red', linestyle='--', label=f'Mean: {np.mean(per_class_f1):.3f}')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Precision\n",
    "axes[1].bar(range(62), per_class_precision, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes[1].set_xticks(range(62))\n",
    "axes[1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Per-Class Precision', fontsize=14, fontweight='bold')\n",
    "axes[1].axhline(y=np.mean(per_class_precision), color='red', linestyle='--', label=f'Mean: {np.mean(per_class_precision):.3f}')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Recall\n",
    "axes[2].bar(range(62), per_class_recall, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes[2].set_xticks(range(62))\n",
    "axes[2].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[2].set_xlabel('Character Class', fontsize=12)\n",
    "axes[2].set_ylabel('Recall', fontsize=12)\n",
    "axes[2].set_title('Per-Class Recall', fontsize=14, fontweight='bold')\n",
    "axes[2].axhline(y=np.mean(per_class_recall), color='red', linestyle='--', label=f'Mean: {np.mean(per_class_recall):.3f}')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best and worst performing classes\n",
    "f1_scores_dict = {class_names[i]: per_class_f1[i] for i in range(len(per_class_f1))}\n",
    "sorted_f1 = sorted(f1_scores_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nBest Performing Classes (Top 10 by F1-Score):\")\n",
    "for char, f1 in sorted_f1[-10:][::-1]:\n",
    "    print(f\"  '{char}': {f1:.4f}\")\n",
    "\n",
    "print(\"\\nWorst Performing Classes (Bottom 10 by F1-Score):\")\n",
    "for char, f1 in sorted_f1[:10]:\n",
    "    print(f\"  '{char}': {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5685f62",
   "metadata": {},
   "source": [
    "## 7. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd01069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correct and incorrect predictions\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "fig.suptitle('Sample Predictions: Correct (Green) vs Incorrect (Red)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Get some correct and incorrect predictions\n",
    "correct_mask = y_pred_labels == y_test_labels\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "correct_indices = np.where(correct_mask)[0]\n",
    "incorrect_indices = np.where(incorrect_mask)[0]\n",
    "\n",
    "# Sample indices\n",
    "np.random.seed(42)\n",
    "sample_correct = np.random.choice(correct_indices, 16, replace=False)\n",
    "sample_incorrect = np.random.choice(incorrect_indices, 16, replace=False)\n",
    "\n",
    "samples = list(sample_correct) + list(sample_incorrect)\n",
    "np.random.shuffle(samples)\n",
    "\n",
    "for idx, sample_idx in enumerate(samples[:32]):\n",
    "    row = idx // 8\n",
    "    col = idx % 8\n",
    "    \n",
    "    image = x_test[sample_idx].reshape(28, 28)\n",
    "    true_label = class_names[y_test_labels[sample_idx]]\n",
    "    pred_label = class_names[y_pred_labels[sample_idx]]\n",
    "    confidence = y_pred_probs[sample_idx][y_pred_labels[sample_idx]] * 100\n",
    "    \n",
    "    is_correct = y_pred_labels[sample_idx] == y_test_labels[sample_idx]\n",
    "    color = 'green' if is_correct else 'red'\n",
    "    \n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    title = f\"T:'{true_label}' P:'{pred_label}'\\n{confidence:.1f}%\"\n",
    "    axes[row, col].set_title(title, fontsize=9, color=color, fontweight='bold')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Correct predictions: {correct_mask.sum():,} ({correct_mask.mean()*100:.2f}%)\")\n",
    "print(f\"  Incorrect predictions: {incorrect_mask.sum():,} ({incorrect_mask.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904fe85",
   "metadata": {},
   "source": [
    "## 8. Confidence Analysis\n",
    "\n",
    "Analyze prediction confidence distribution to understand model certainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd70ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "max_confidences = y_pred_probs.max(axis=1)\n",
    "correct_confidences = max_confidences[correct_mask]\n",
    "incorrect_confidences = max_confidences[incorrect_mask]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Overall confidence distribution\n",
    "axes[0].hist(max_confidences, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(max_confidences.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {max_confidences.mean():.3f}')\n",
    "axes[0].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Overall Prediction Confidence Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Correct vs incorrect confidence\n",
    "axes[1].hist([correct_confidences, incorrect_confidences], bins=50, alpha=0.7, \n",
    "             label=['Correct', 'Incorrect'], color=['green', 'red'], edgecolor='black')\n",
    "axes[1].set_xlabel('Confidence', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Confidence: Correct vs Incorrect Predictions', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "box_data = [correct_confidences, incorrect_confidences]\n",
    "bp = axes[2].boxplot(box_data, labels=['Correct', 'Incorrect'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightgreen')\n",
    "bp['boxes'][1].set_facecolor('lightcoral')\n",
    "axes[2].set_ylabel('Confidence', fontsize=12)\n",
    "axes[2].set_title('Confidence Distribution Comparison', fontsize=13, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confidence statistics\n",
    "print(\"\\nConfidence Statistics:\")\n",
    "print(f\"  Overall mean confidence: {max_confidences.mean():.4f}\")\n",
    "print(f\"  Correct predictions mean confidence: {correct_confidences.mean():.4f}\")\n",
    "print(f\"  Incorrect predictions mean confidence: {incorrect_confidences.mean():.4f}\")\n",
    "print(f\"\\n  Low confidence correct (<0.5): {(correct_confidences < 0.5).sum():,}\")\n",
    "print(f\"  High confidence incorrect (>0.8): {(incorrect_confidences > 0.8).sum():,}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
